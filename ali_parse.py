import json
import os
from datetime import datetime
import logging
import asyncio
import random
import time

import aiohttp
import requests
from data import (
    get_item_info,
    get_shopify_one_item,
    get_items_list_from_query,
    save_json,
    save_csv,
    save_shopify_csv_one_item,
    save_shopify_csv_list_items,
)
from hosting import upload_photos
from dotenv import load_dotenv

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞
load_dotenv()

headers = {
    "x-rapidapi-key": os.getenv('RAPID_API_KEY'),
    "x-rapidapi-host": "aliexpress-datahub.p.rapidapi.com",
}

# –î–æ–¥–∞—î–º–æ –∑–∞—Ç—Ä–∏–º–∫—É –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
async def delay_request():
    await asyncio.sleep(random.uniform(2, 4))

async def make_request(url: str, params: dict) -> dict:
    """–í–∏–∫–æ–Ω—É—î HTTP –∑–∞–ø–∏—Ç –∑ –ø–æ–≤—Ç–æ—Ä–Ω–∏–º–∏ —Å–ø—Ä–æ–±–∞–º–∏ —Ç–∞ –æ–±—Ä–æ–±–∫–æ—é –ø–æ–º–∏–ª–æ–∫"""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            await delay_request()
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, params=params, timeout=30) as response:
                    if response.status == 429:
                        wait_time = int(response.headers.get('Retry-After', 60))
                        logging.warning(f"Rate limit reached. Waiting {wait_time} seconds...")
                        await asyncio.sleep(wait_time)
                        continue
                    response.raise_for_status()
                    return await response.json()
        except aiohttp.ClientError as e:
            if attempt == max_retries - 1:
                logging.error(f"Failed after {max_retries} attempts: {str(e)}")
                return None
            await asyncio.sleep(5 * (attempt + 1))
    return None

async def parse_item(headers: dict, item_id: str) -> tuple[dict, dict] | None:
    """–ü–æ–≤–µ—Ä—Ç–∞—î –¥–∞–Ω—ñ –ø—Ä–æ —Ç–æ–≤–∞—Ä –∑–∞ ID —ñ–∑ —Å–∞–π—Ç—É."""
    url = "https://aliexpress-datahub.p.rapidapi.com/item_detail_7"
    url_reviews = "https://aliexpress-datahub.p.rapidapi.com/item_review"

    querystring = {"itemId": item_id, "region": "US"}
    querystring_reviews = {"itemId": item_id, "page": "1", "sort": "default", "filter": "allReviews"}

    data_item = await make_request(url, querystring)
    if not data_item or data_item.get("result", {}).get("status", {}).get("data") == "error":
        return None

    await delay_request()
    data_reviews = await make_request(url_reviews, querystring_reviews)
    if not data_reviews or data_reviews.get("result", {}).get("status", {}).get("data") == "error":
        data_reviews = None

    return data_item, data_reviews

def get_item_id_from_url(link: str) -> str:
    """–ü–æ–≤–µ—Ä—Ç–∞—î ID —Ç–æ–≤–∞—Ä—É –∑ –ø–æ—Å–∏–ª–∞–Ω–Ω—è."""
    try:
        if "item/" in link:
            item_id = link.split("item/")[1].split(".")[0]
        elif "/_i/" in link:
            item_id = link.split("/_i/")[1].split(".")[0]
        else:
            return ""
        return item_id.strip()
    except (IndexError, AttributeError):
        return ""

def get_query_from_url(url: str) -> str:
    """–í–∏—Ç—è–≥—É—î –ø–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç –∑ URL AliExpress"""
    try:
        # –û—á–∏—â–∞—î–º–æ URL
        url = url.lower().strip()
        
        # –°–ø—Ä–æ–±—É—î–º–æ –∑–Ω–∞–π—Ç–∏ –ø–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç –≤ —Ä—ñ–∑–Ω–∏—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö URL
        if 'wholesale-' in url:
            query = url.split('wholesale-')[1].split('.')[0]
            logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –ø–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç –≤ URL: {query}")
            return query.replace('-', ' ')
        elif 'SearchText=' in url:
            query = url.split('SearchText=')[1].split('&')[0]
            logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –ø–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç –≤ URL: {query}")
            return query
        elif '/search/' in url:
            query = url.split('/search/')[1].split('.html')[0]
            logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –ø–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç –≤ URL: {query}")
            return query
        else:
            # –Ø–∫—â–æ —Ü–µ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤–∏–π –∑–∞–ø–∏—Ç
            logging.info(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–µ–∫—Å—Ç–æ–≤–∏–π –∑–∞–ø–∏—Ç: {url}")
            return url
            
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–æ—à—É–∫–æ–≤–æ–≥–æ –∑–∞–ø–∏—Ç—É: {e}")
        return url

async def parse_query(headers: dict, query: str) -> dict:
    """–ü–æ–≤–µ—Ä—Ç–∞—î –¥–∞–Ω—ñ –ø—Ä–æ —Ç–æ–≤–∞—Ä–∏ –∑–∞ –ø–æ—à—É–∫–æ–≤–∏–º –∑–∞–ø–∏—Ç–æ–º."""
    url_query = "https://aliexpress-datahub.p.rapidapi.com/item_search_4"
    
    # –û—á–∏—â–∞—î–º–æ —Ç–∞ —Ñ–æ—Ä–º–∞—Ç—É—î–º–æ –ø–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç
    clean_query = query.replace("+", " ").strip()
    
    if "aliexpress" in clean_query.lower():
        try:
            # –Ø–∫—â–æ —Ü–µ URL, –≤–∏—Ç—è–≥—É—î–º–æ –ø–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç
            if "SearchText=" in clean_query:
                clean_query = clean_query.split("SearchText=")[1].split("&")[0]
            elif "wholesale-" in clean_query:
                clean_query = clean_query.split("wholesale-")[1].split(".")[0]
                clean_query = clean_query.replace("-", " ")
            elif "w/wholesale-" in clean_query:
                clean_query = clean_query.split("wholesale-")[1].split(".")[0]
                clean_query = clean_query.replace("-", " ")
            elif "keywords=" in clean_query:
                clean_query = clean_query.split("keywords=")[1].split("&")[0]
                
            # –î–µ–∫–æ–¥—É—î–º–æ URL-encoded —Å–∏–º–≤–æ–ª–∏
            clean_query = requests.utils.unquote(clean_query)
            clean_query = clean_query.replace("-", " ")
            
        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ URL: {e}")
            return {}
    
    logging.info(f"–ü–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç: {clean_query}")
    
    querystring = {
        "q": clean_query,
        "page": "1", 
        "sort": "total_tranpro_desc",  # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ –ø—Ä–æ–¥–∞–∂–∞–º–∏
        "region": "US",
        "shipTo": "US",
        "size": "50"
    }
    
    try:
        data = await make_request(url_query, querystring)
        if not data:
            logging.error("–ù–µ –æ—Ç—Ä–∏–º–∞–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –≤—ñ–¥ API")
            return {}
            
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        items = data.get("result", {}).get("resultList", [])
        if not items:
            logging.error("–ù–µ–º–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ—à—É–∫—É")
            return {}
            
        logging.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(items)} —Ç–æ–≤–∞—Ä—ñ–≤")
        return data
        
    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É —Ç–æ–≤–∞—Ä—ñ–≤: {e}")
        return {}

def get_items_list_from_query(items: dict) -> list:
    """–ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ ID —Ç–æ–≤–∞—Ä—ñ–≤ —ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ—à—É–∫–æ–≤–æ–≥–æ –∑–∞–ø–∏—Ç—É."""
    try:
        result_list = items.get('result', {}).get('resultList', [])
        item_ids = []
        for item in result_list:
            if isinstance(item, dict):
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä—ñ–∑–Ω—ñ –º–æ–∂–ª–∏–≤—ñ —à–ª—è—Ö–∏ –¥–æ ID —Ç–æ–≤–∞—Ä—É
                item_id = (
                    item.get('productId') or 
                    item.get('item', {}).get('productId') or
                    item.get('item', {}).get('itemId')
                )
                if item_id:
                    item_ids.append(str(item_id))
        return item_ids
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É —Ç–æ–≤–∞—Ä—ñ–≤: {str(e)}")
        return []

def parse_item_from_link(link: str) -> None:
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –æ–¥–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É –∑–∞ –ø–æ—Å–∏–ª–∞–Ω–Ω—è–º."""
    item_id = get_item_id_from_url(link)
    item_data = parse_item(headers, item_id)
    if item_data:
        item_dict = get_item_info(item_data)
        save_json(item_dict, item_id)
        save_csv(item_dict.copy(), item_id)
        main_photos_url = upload_photos(item_dict["MainPhotoLinks"], f"{item_id}/MainPhotos")
        upload_photos(item_dict["ReviewsPhotoLinks"], f"{item_id}/PhotoReview")
        shopify_info = get_shopify_one_item(item_dict, main_photos_url)
        save_shopify_csv_one_item(shopify_info, item_id)

def parse_items_from_links(headers: dict, items_id: list, filename: str = "list_items") -> bool:
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –±–∞–≥–∞—Ç—å–æ—Ö —Ç–æ–≤–∞—Ä—ñ–≤ —ñ–∑ —Å–ø–∏—Å–∫—É."""
    try:
        items = []
        shopify_list = []
        for item_id in items_id:
            try:
                item_data = parse_item(headers, str(item_id))  # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ ID –≤ —Ä—è–¥–æ–∫
                if item_data:
                    item_dict = get_item_info(item_data)
                    if item_dict:
                        items.append(item_dict)
                        main_photos_url = upload_photos(item_dict.get("MainPhotoLinks", []), f"{item_id}/MainPhotos")
                        if item_dict.get("ReviewsPhotoLinks"):
                            upload_photos(item_dict["ReviewsPhotoLinks"], f"{item_id}/PhotoReview")
                        shopify_info = get_shopify_one_item(item_dict, main_photos_url)
                        shopify_list.append(shopify_info)
            except Exception as e:
                logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥—É —Ç–æ–≤–∞—Ä—É {item_id}: {str(e)}")
                continue
        if items and shopify_list:
            save_json(items, filename)
            save_csv(items, filename)
            save_shopify_csv_list_items(shopify_list, filename)
            return True
        return False
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥—É —Å–ø–∏—Å–∫—É —Ç–æ–≤–∞—Ä—ñ–≤: {str(e)}")
        return False

async def parse_items_from_query(headers: dict, query: str, items_count: int, log_callback=None, folder_name="list_items") -> bool:
    """–ü–∞—Ä—Å–∏–Ω–≥ –±–∞–≥–∞—Ç—å–æ—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –∑–∞ –ø–æ—à—É–∫–æ–≤–∏–º –∑–∞–ø–∏—Ç–æ–º."""
    try:
        async def log(text: str):
            logging.info(text)
            if log_callback:
                await log_callback(text)
        
        await log("üîç –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ—à—É–∫–æ–≤–æ–≥–æ –∑–∞–ø–∏—Ç—É...")
        
        if "aliexpress" in query.lower():
            query = get_query_from_url(query)
            if not query:
                await log("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –ø–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç –∑ URL")
                return False
            await log(f"‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç: {query}")
        
        await log("üîç –ü–æ—à—É–∫ —Ç–æ–≤–∞—Ä—ñ–≤...")
        query_data = await parse_query(headers, query)
        if not query_data:
            await log("‚ùå –ù–µ –æ—Ç—Ä–∏–º–∞–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ—à—É–∫—É")
            return False
            
        items_list = get_items_list_from_query(query_data)
        if not items_list:
            await log("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤")
            return False
            
        await log(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(items_list)} —Ç–æ–≤–∞—Ä—ñ–≤")
        await log(f"‚öôÔ∏è –û–±—Ä–æ–±–∫–∞ –ø–µ—Ä—à–∏—Ö {items_count} —Ç–æ–≤–∞—Ä—ñ–≤")
        
        # –ü–∞—Ä—Å–∏–º–æ —Ç–æ–≤–∞—Ä–∏
        items_data = []
        shopify_list = []
        
        for idx, item_id in enumerate(items_list[:items_count], 1):
            try:
                await log(f"üì¶ –û–±—Ä–æ–±–∫–∞ —Ç–æ–≤–∞—Ä—É {idx}/{items_count}")
                item_data = await parse_item(headers, item_id)
                if not item_data:
                    await log(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö —Ç–æ–≤–∞—Ä—É {idx}")
                    continue
                    
                item_dict = get_item_info(item_data)
                if item_dict:
                    items_data.append(item_dict)
                    await log(f"üì∏ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–æ—Ç–æ —Ç–æ–≤–∞—Ä—É {idx}")
                    main_photos_url = upload_photos(
                        item_dict["MainPhotoLinks"],
                        f"{item_id}/MainPhotos"
                    )
                    if item_dict["ReviewsPhotoLinks"]:
                        upload_photos(
                            item_dict["ReviewsPhotoLinks"],
                            f"{item_id}/PhotoReview"
                        )
                    shopify_info = get_shopify_one_item(item_dict, main_photos_url)
                    shopify_list.append(shopify_info)
                    await log(f"‚úÖ –¢–æ–≤–∞—Ä {idx} —É—Å–ø—ñ—à–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ")
                    
            except Exception as e:
                await log(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Ç–æ–≤–∞—Ä—É {idx}: {e}")
                continue
        
        if items_data and shopify_list:
            await log("üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤...")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_path = os.path.join(folder_name, f"items_{timestamp}")
            
            try:
                save_json(items_data, base_path)
                save_csv(items_data, base_path)
                save_shopify_csv_list_items(shopify_list, base_path)
                await log(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(items_data)} —Ç–æ–≤–∞—Ä—ñ–≤")
                return True
            except Exception as e:
                await log(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤: {e}")
                return False
            
        return False
        
    except Exception as e:
        if log_callback:
            await log(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥—É –∑–∞–ø–∏—Ç—É: {e}")
        return False
